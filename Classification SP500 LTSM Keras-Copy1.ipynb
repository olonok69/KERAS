{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "j:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "j:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Classification Stock Prices  with Keras LTSM Network\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as wb\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import library as mio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduced list only the most correlated\n",
    "indices_list_Complete = [\"SPY\",\"^IXIC\", \"^DJI\", \"^GDAXI\", \"^FTSE\",\"^FCHI\", \"^N225\",\"^HSI\", \"^AXJO\",\"ORB\", \"EUR\",\"AUD\",\"GBP\",\"JPY\", \"SILVER\", \"GOLD\", \"WT1010\"] # reduced list only the most correlated\n",
    "indice_target=[\"^GSPC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date = \"2003-01-01\" # Start day of Series\n",
    "end_date = \"2017-01-01\"\t  # Final day of series\n",
    "dates = pd.date_range(start_date, end_date)  # date range as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_index = mio.get_data(indices_list_Complete, dates)\n",
    "df_target= mio.get_data(indice_target, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_index.fillna(method='bfill', inplace=True)\n",
    "df_target.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY</th>\n",
       "      <th>^IXIC</th>\n",
       "      <th>^DJI</th>\n",
       "      <th>^GDAXI</th>\n",
       "      <th>^FTSE</th>\n",
       "      <th>^FCHI</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^HSI</th>\n",
       "      <th>^AXJO</th>\n",
       "      <th>ORB</th>\n",
       "      <th>EUR</th>\n",
       "      <th>AUD</th>\n",
       "      <th>GBP</th>\n",
       "      <th>JPY</th>\n",
       "      <th>SILVER</th>\n",
       "      <th>GOLD</th>\n",
       "      <th>WT1010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>124.247170</td>\n",
       "      <td>2887.485241</td>\n",
       "      <td>12676.749688</td>\n",
       "      <td>6792.847443</td>\n",
       "      <td>5650.849134</td>\n",
       "      <td>4130.124744</td>\n",
       "      <td>12944.451194</td>\n",
       "      <td>19392.780442</td>\n",
       "      <td>4716.324737</td>\n",
       "      <td>70.492837</td>\n",
       "      <td>0.784911</td>\n",
       "      <td>1.218466</td>\n",
       "      <td>0.605921</td>\n",
       "      <td>103.269039</td>\n",
       "      <td>16.829385</td>\n",
       "      <td>993.989504</td>\n",
       "      <td>2.667827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.716304</td>\n",
       "      <td>1111.738679</td>\n",
       "      <td>3019.752104</td>\n",
       "      <td>2287.728860</td>\n",
       "      <td>879.576705</td>\n",
       "      <td>772.017335</td>\n",
       "      <td>3478.880031</td>\n",
       "      <td>4632.425945</td>\n",
       "      <td>855.962899</td>\n",
       "      <td>29.223703</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.180926</td>\n",
       "      <td>0.066218</td>\n",
       "      <td>13.772426</td>\n",
       "      <td>8.778454</td>\n",
       "      <td>427.903735</td>\n",
       "      <td>1.383519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>57.440272</td>\n",
       "      <td>1268.640015</td>\n",
       "      <td>6547.049805</td>\n",
       "      <td>2202.959961</td>\n",
       "      <td>3287.000000</td>\n",
       "      <td>2403.040039</td>\n",
       "      <td>7054.979980</td>\n",
       "      <td>8409.009766</td>\n",
       "      <td>2700.399902</td>\n",
       "      <td>22.480000</td>\n",
       "      <td>0.625360</td>\n",
       "      <td>0.906698</td>\n",
       "      <td>0.474330</td>\n",
       "      <td>75.751979</td>\n",
       "      <td>4.370000</td>\n",
       "      <td>319.750000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>93.528227</td>\n",
       "      <td>2094.139893</td>\n",
       "      <td>10444.370117</td>\n",
       "      <td>4998.160156</td>\n",
       "      <td>5053.200195</td>\n",
       "      <td>3580.479980</td>\n",
       "      <td>9844.589844</td>\n",
       "      <td>15413.429688</td>\n",
       "      <td>4169.500000</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>0.734626</td>\n",
       "      <td>1.071435</td>\n",
       "      <td>0.550249</td>\n",
       "      <td>92.727524</td>\n",
       "      <td>10.770000</td>\n",
       "      <td>611.250000</td>\n",
       "      <td>1.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>112.113904</td>\n",
       "      <td>2480.330078</td>\n",
       "      <td>12105.549805</td>\n",
       "      <td>6534.970215</td>\n",
       "      <td>5808.799805</td>\n",
       "      <td>4038.699951</td>\n",
       "      <td>11944.299805</td>\n",
       "      <td>20672.390625</td>\n",
       "      <td>4807.399902</td>\n",
       "      <td>66.340000</td>\n",
       "      <td>0.772966</td>\n",
       "      <td>1.237935</td>\n",
       "      <td>0.616478</td>\n",
       "      <td>105.553327</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>1081.100000</td>\n",
       "      <td>3.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.589273</td>\n",
       "      <td>3496.429932</td>\n",
       "      <td>15112.190430</td>\n",
       "      <td>8260.480469</td>\n",
       "      <td>6364.700195</td>\n",
       "      <td>4582.830078</td>\n",
       "      <td>16024.849609</td>\n",
       "      <td>22835.820312</td>\n",
       "      <td>5342.399902</td>\n",
       "      <td>101.890000</td>\n",
       "      <td>0.829636</td>\n",
       "      <td>1.341054</td>\n",
       "      <td>0.643910</td>\n",
       "      <td>115.820209</td>\n",
       "      <td>20.020000</td>\n",
       "      <td>1308.250000</td>\n",
       "      <td>3.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>225.444643</td>\n",
       "      <td>5487.439941</td>\n",
       "      <td>19974.619141</td>\n",
       "      <td>12374.730469</td>\n",
       "      <td>7142.799805</td>\n",
       "      <td>6168.149902</td>\n",
       "      <td>20868.029297</td>\n",
       "      <td>31638.220703</td>\n",
       "      <td>6828.700195</td>\n",
       "      <td>140.730000</td>\n",
       "      <td>0.963904</td>\n",
       "      <td>1.776110</td>\n",
       "      <td>0.822470</td>\n",
       "      <td>125.630900</td>\n",
       "      <td>48.700000</td>\n",
       "      <td>1891.000000</td>\n",
       "      <td>4.690000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               SPY        ^IXIC          ^DJI        ^GDAXI        ^FTSE  \\\n",
       "count  3525.000000  3525.000000   3525.000000   3525.000000  3525.000000   \n",
       "mean    124.247170  2887.485241  12676.749688   6792.847443  5650.849134   \n",
       "std      42.716304  1111.738679   3019.752104   2287.728860   879.576705   \n",
       "min      57.440272  1268.640015   6547.049805   2202.959961  3287.000000   \n",
       "25%      93.528227  2094.139893  10444.370117   4998.160156  5053.200195   \n",
       "50%     112.113904  2480.330078  12105.549805   6534.970215  5808.799805   \n",
       "75%     152.589273  3496.429932  15112.190430   8260.480469  6364.700195   \n",
       "max     225.444643  5487.439941  19974.619141  12374.730469  7142.799805   \n",
       "\n",
       "             ^FCHI         ^N225          ^HSI        ^AXJO          ORB  \\\n",
       "count  3525.000000   3525.000000   3525.000000  3525.000000  3525.000000   \n",
       "mean   4130.124744  12944.451194  19392.780442  4716.324737    70.492837   \n",
       "std     772.017335   3478.880031   4632.425945   855.962899    29.223703   \n",
       "min    2403.040039   7054.979980   8409.009766  2700.399902    22.480000   \n",
       "25%    3580.479980   9844.589844  15413.429688  4169.500000    45.250000   \n",
       "50%    4038.699951  11944.299805  20672.390625  4807.399902    66.340000   \n",
       "75%    4582.830078  16024.849609  22835.820312  5342.399902   101.890000   \n",
       "max    6168.149902  20868.029297  31638.220703  6828.700195   140.730000   \n",
       "\n",
       "               EUR          AUD          GBP          JPY       SILVER  \\\n",
       "count  3525.000000  3525.000000  3525.000000  3525.000000  3525.000000   \n",
       "mean      0.784911     1.218466     0.605921   103.269039    16.829385   \n",
       "std       0.073974     0.180926     0.066218    13.772426     8.778454   \n",
       "min       0.625360     0.906698     0.474330    75.751979     4.370000   \n",
       "25%       0.734626     1.071435     0.550249    92.727524    10.770000   \n",
       "50%       0.772966     1.237935     0.616478   105.553327    15.860000   \n",
       "75%       0.829636     1.341054     0.643910   115.820209    20.020000   \n",
       "max       0.963904     1.776110     0.822470   125.630900    48.700000   \n",
       "\n",
       "              GOLD       WT1010  \n",
       "count  3525.000000  3525.000000  \n",
       "mean    993.989504     2.667827  \n",
       "std     427.903735     1.383519  \n",
       "min     319.750000    -0.200000  \n",
       "25%     611.250000     1.520000  \n",
       "50%    1081.100000     3.150000  \n",
       "75%    1308.250000     3.910000  \n",
       "max    1891.000000     4.690000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colums_1=df_index.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in colums_1:\n",
    "    df_index[index+'_Ret'] = df_index[index].pct_change(1)\n",
    "    df_index.drop(index,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_index.replace([np.inf, -np.inf], np.nan)\n",
    "df_index[df_index==np.inf] = np.nan\n",
    "df_index[df_index==-np.inf] = np.nan\n",
    "df_index.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_Ret</th>\n",
       "      <th>^IXIC_Ret</th>\n",
       "      <th>^DJI_Ret</th>\n",
       "      <th>^GDAXI_Ret</th>\n",
       "      <th>^FTSE_Ret</th>\n",
       "      <th>^FCHI_Ret</th>\n",
       "      <th>^N225_Ret</th>\n",
       "      <th>^HSI_Ret</th>\n",
       "      <th>^AXJO_Ret</th>\n",
       "      <th>ORB_Ret</th>\n",
       "      <th>EUR_Ret</th>\n",
       "      <th>AUD_Ret</th>\n",
       "      <th>GBP_Ret</th>\n",
       "      <th>JPY_Ret</th>\n",
       "      <th>SILVER_Ret</th>\n",
       "      <th>GOLD_Ret</th>\n",
       "      <th>WT1010_Ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "      <td>3525.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>-0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011829</td>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.011625</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.014816</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>0.183213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.098448</td>\n",
       "      <td>-0.091424</td>\n",
       "      <td>-0.078733</td>\n",
       "      <td>-0.074472</td>\n",
       "      <td>-0.088483</td>\n",
       "      <td>-0.090368</td>\n",
       "      <td>-0.114064</td>\n",
       "      <td>-0.136666</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.084716</td>\n",
       "      <td>-0.031090</td>\n",
       "      <td>-0.049097</td>\n",
       "      <td>-0.031450</td>\n",
       "      <td>-0.036115</td>\n",
       "      <td>-0.170495</td>\n",
       "      <td>-0.085271</td>\n",
       "      <td>-3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.004187</td>\n",
       "      <td>-0.005483</td>\n",
       "      <td>-0.004262</td>\n",
       "      <td>-0.006168</td>\n",
       "      <td>-0.005039</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>-0.005922</td>\n",
       "      <td>-0.004710</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>-0.003242</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>-0.003059</td>\n",
       "      <td>-0.003001</td>\n",
       "      <td>-0.010110</td>\n",
       "      <td>-0.005659</td>\n",
       "      <td>-0.011538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.012328</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.145198</td>\n",
       "      <td>0.118059</td>\n",
       "      <td>0.110803</td>\n",
       "      <td>0.114020</td>\n",
       "      <td>0.098387</td>\n",
       "      <td>0.111762</td>\n",
       "      <td>0.141503</td>\n",
       "      <td>0.143471</td>\n",
       "      <td>0.057896</td>\n",
       "      <td>0.136594</td>\n",
       "      <td>0.032361</td>\n",
       "      <td>0.096461</td>\n",
       "      <td>0.082558</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.100255</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SPY_Ret    ^IXIC_Ret     ^DJI_Ret   ^GDAXI_Ret    ^FTSE_Ret  \\\n",
       "count  3525.000000  3525.000000  3525.000000  3525.000000  3525.000000   \n",
       "mean      0.000405     0.000472     0.000296     0.000471     0.000231   \n",
       "std       0.011829     0.013110     0.011018     0.014207     0.011625   \n",
       "min      -0.098448    -0.091424    -0.078733    -0.074472    -0.088483   \n",
       "25%      -0.004187    -0.005483    -0.004262    -0.006168    -0.005039   \n",
       "50%       0.000730     0.000928     0.000505     0.000837     0.000307   \n",
       "75%       0.005602     0.006871     0.005288     0.007436     0.005789   \n",
       "max       0.145198     0.118059     0.110803     0.114020     0.098387   \n",
       "\n",
       "         ^FCHI_Ret    ^N225_Ret     ^HSI_Ret    ^AXJO_Ret      ORB_Ret  \\\n",
       "count  3525.000000  3525.000000  3525.000000  3525.000000  3525.000000   \n",
       "mean      0.000220     0.000341     0.000359     0.000236     0.000321   \n",
       "std       0.014227     0.015348     0.014816     0.010559     0.017367   \n",
       "min      -0.090368    -0.114064    -0.136666    -0.097524    -0.084716   \n",
       "25%      -0.006589    -0.006627    -0.005922    -0.004710    -0.008782   \n",
       "50%       0.000340     0.000000     0.000000     0.000368     0.000543   \n",
       "75%       0.007399     0.008219     0.007064     0.005665     0.009836   \n",
       "max       0.111762     0.141503     0.143471     0.057896     0.136594   \n",
       "\n",
       "           EUR_Ret      AUD_Ret      GBP_Ret      JPY_Ret   SILVER_Ret  \\\n",
       "count  3525.000000  3525.000000  3525.000000  3525.000000  3525.000000   \n",
       "mean      0.000011    -0.000040     0.000089     0.000009     0.000604   \n",
       "std       0.005865     0.007932     0.005748     0.005649     0.022397   \n",
       "min      -0.031090    -0.049097    -0.031450    -0.036115    -0.170495   \n",
       "25%      -0.003242    -0.004329    -0.003059    -0.003001    -0.010110   \n",
       "50%      -0.000115    -0.000408    -0.000003     0.000026     0.000000   \n",
       "75%       0.003206     0.003811     0.002998     0.003140     0.012328   \n",
       "max       0.032361     0.096461     0.082558     0.031060     0.200557   \n",
       "\n",
       "          GOLD_Ret   WT1010_Ret  \n",
       "count  3525.000000  3525.000000  \n",
       "mean      0.000420    -0.002273  \n",
       "std       0.012036     0.183213  \n",
       "min      -0.085271    -3.000000  \n",
       "25%      -0.005659    -0.011538  \n",
       "50%       0.000317     0.000000  \n",
       "75%       0.006895     0.009804  \n",
       "max       0.100255     4.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colums_2=df_target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in colums_2:\n",
    "    df_target[index+'_Ret'] = df_target[index].pct_change(1)\n",
    "    df_target.drop(index,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_target.replace([np.inf, -np.inf], np.nan)\n",
    "df_target[df_target==np.inf] = np.nan\n",
    "df_target[df_target==-np.inf] = np.nan\n",
    "df_target.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^GSPC_Ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3525.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.090350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.004445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.005545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ^GSPC_Ret\n",
       "count  3525.000000\n",
       "mean      0.000326\n",
       "std       0.011888\n",
       "min      -0.090350\n",
       "25%      -0.004445\n",
       "50%       0.000693\n",
       "75%       0.005545\n",
       "max       0.115800"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final_1 = df_index.join(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_final_1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Label_Change2 (x):\n",
    "    if x >= 0 :\n",
    "        return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_final.applymap(Label_Change2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns=df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 3525\n",
      "Training Observations: 2326\n",
      "Testing Observations: 1199\n"
     ]
    }
   ],
   "source": [
    "values = df_final.values\n",
    "train_size = int(len(values) * 0.66)\n",
    "train, test = values[0:train_size], values[train_size:len(values)]\n",
    "print('Observations: %d' % (len(values)))\n",
    "print('Training Observations: %d' % (len(train)))\n",
    "print('Testing Observations: %d' % (len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test=df_final[train_size:len(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexs=df_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-03-28', '2012-03-29', '2012-03-30', '2012-04-02',\n",
       "               '2012-04-03', '2012-04-04', '2012-04-05', '2012-04-09',\n",
       "               '2012-04-10', '2012-04-11',\n",
       "               ...\n",
       "               '2016-12-16', '2016-12-19', '2016-12-20', '2016-12-21',\n",
       "               '2016-12-22', '2016-12-23', '2016-12-27', '2016-12-28',\n",
       "               '2016-12-29', '2016-12-30'],\n",
       "              dtype='datetime64[ns]', length=1199, freq=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2326, 17) (2326,) (1199, 17) (1199,)\n"
     ]
    }
   ],
   "source": [
    "#train_XS = train_XS.reshape((train_XS.shape[0], 1, train_XS.shape[1]))\n",
    "#test_XS = test_XS.reshape((test_XS.shape[0], 1, test_XS.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "# play around with learning rate and optimizer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=17, activation='sigmoid'))\n",
    "model.compile(Adam(lr=0.05), 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 18\n",
      "Trainable params: 18\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2326 samples, validate on 1199 samples\n",
      "Epoch 1/250\n",
      " - 0s - loss: 0.3670 - acc: 0.8672 - val_loss: 0.2179 - val_acc: 0.9283\n",
      "Epoch 2/250\n",
      " - 0s - loss: 0.1910 - acc: 0.9407 - val_loss: 0.1565 - val_acc: 0.9516\n",
      "Epoch 3/250\n",
      " - 0s - loss: 0.1626 - acc: 0.9454 - val_loss: 0.1383 - val_acc: 0.9550\n",
      "Epoch 4/250\n",
      " - 0s - loss: 0.1545 - acc: 0.9463 - val_loss: 0.1296 - val_acc: 0.9558\n",
      "Epoch 5/250\n",
      " - 0s - loss: 0.1508 - acc: 0.9471 - val_loss: 0.1245 - val_acc: 0.9558\n",
      "Epoch 6/250\n",
      " - 0s - loss: 0.1488 - acc: 0.9475 - val_loss: 0.1213 - val_acc: 0.9558\n",
      "Epoch 7/250\n",
      " - 0s - loss: 0.1477 - acc: 0.9475 - val_loss: 0.1192 - val_acc: 0.9550\n",
      "Epoch 8/250\n",
      " - 0s - loss: 0.1471 - acc: 0.9471 - val_loss: 0.1178 - val_acc: 0.9558\n",
      "Epoch 9/250\n",
      " - 0s - loss: 0.1467 - acc: 0.9475 - val_loss: 0.1168 - val_acc: 0.9558\n",
      "Epoch 10/250\n",
      " - 0s - loss: 0.1464 - acc: 0.9475 - val_loss: 0.1161 - val_acc: 0.9566\n",
      "Epoch 11/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9480 - val_loss: 0.1156 - val_acc: 0.9558\n",
      "Epoch 12/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9484 - val_loss: 0.1152 - val_acc: 0.9558\n",
      "Epoch 13/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9475 - val_loss: 0.1149 - val_acc: 0.9566\n",
      "Epoch 14/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9471 - val_loss: 0.1147 - val_acc: 0.9566\n",
      "Epoch 15/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1146 - val_acc: 0.9566\n",
      "Epoch 16/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1144 - val_acc: 0.9566\n",
      "Epoch 17/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1143 - val_acc: 0.9566\n",
      "Epoch 18/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1143 - val_acc: 0.9566\n",
      "Epoch 19/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1142 - val_acc: 0.9566\n",
      "Epoch 20/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1141 - val_acc: 0.9566\n",
      "Epoch 21/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9471 - val_loss: 0.1141 - val_acc: 0.9566\n",
      "Epoch 22/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9471 - val_loss: 0.1141 - val_acc: 0.9566\n",
      "Epoch 23/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9471 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 24/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 25/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 26/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 27/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 28/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 29/250\n",
      " - 0s - loss: 0.1459 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 30/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 31/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 32/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 33/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 34/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 35/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 36/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 37/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 38/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 39/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 40/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 41/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 42/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 43/250\n",
      " - 0s - loss: 0.1460 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 44/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 45/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 46/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 47/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 48/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 49/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 50/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 51/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 52/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 53/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 54/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1139 - val_acc: 0.9566\n",
      "Epoch 55/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 56/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 57/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 58/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 59/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 60/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 61/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 62/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 63/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 64/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 65/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 66/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 67/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 68/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 69/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 70/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 71/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 72/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 73/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 74/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 75/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 76/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 77/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 78/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 79/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 80/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 81/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 82/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 83/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 84/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 85/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 86/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 87/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 88/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 89/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 90/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 91/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 92/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 93/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 94/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 95/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 96/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 98/250\n",
      " - 0s - loss: 0.1461 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 99/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 100/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 101/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 102/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 103/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 104/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 105/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 106/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 107/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 108/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 109/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 110/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 111/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 112/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 113/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 114/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 115/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 116/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 117/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 118/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 119/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 120/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 121/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 122/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 123/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 124/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 125/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 126/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 127/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 128/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 129/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 130/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 131/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 132/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 133/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 134/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 135/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 136/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 137/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 138/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 139/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 140/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 141/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 142/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 143/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 144/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 145/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 146/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 147/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 148/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 149/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 150/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 151/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 152/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 153/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 154/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 155/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 156/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 157/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 158/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 159/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 160/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 161/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 162/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 163/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 164/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 165/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 166/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 167/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 168/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 169/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 170/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 171/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 172/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 173/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 174/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 175/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 176/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 177/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 178/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 179/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 180/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 181/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 182/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 183/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 184/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 185/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 186/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 187/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 188/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 189/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 190/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 191/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 192/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 194/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 195/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 196/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 197/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 198/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 199/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 200/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 201/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 202/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 203/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 204/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 205/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 206/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 207/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 208/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 209/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 210/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 211/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 212/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 213/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 214/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 215/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 216/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 217/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 218/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 219/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 220/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 221/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 222/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 223/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 224/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 225/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 226/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 227/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 228/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 229/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 230/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 231/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 232/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 233/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 234/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 235/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 236/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 237/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 238/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 239/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 240/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 241/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 242/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 243/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 244/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 245/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 246/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 247/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 248/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 249/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n",
      "Epoch 250/250\n",
      " - 0s - loss: 0.1462 - acc: 0.9467 - val_loss: 0.1140 - val_acc: 0.9566\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, verbose=2, shuffle=False, validation_data=(test_X, test_y), epochs=250, batch_size=64)\n",
    "#model.fit(train_XS, train_yS, epochs=250, batch_size=16, validation_data=(test_XS, test_yS), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eeee2df048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# plot history\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "y_test_pred = model.predict_classes(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_confusion_matrix(y_true, y_pred, labels=[\"False\", \"True\"]):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pred_labels = ['Predicted '+ l for l in labels]\n",
    "    df = pd.DataFrame(cm, index=labels, columns=pred_labels)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted UP</th>\n",
       "      <th>Predicted Down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UP</th>\n",
       "      <td>529</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Down</th>\n",
       "      <td>17</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted UP  Predicted Down\n",
       "UP             529              35\n",
       "Down            17             618"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the confusion matrix, precision and recall\n",
    "\n",
    "pretty_confusion_matrix(test_y, y_test_pred, labels=['UP', 'Down'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95       564\n",
      "          1       0.95      0.97      0.96       635\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
